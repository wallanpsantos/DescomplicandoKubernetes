
<!DOCTYPE HTML>
<html lang="pt" >
    <head>
        <meta charset="UTF-8">
        <title>Descomplicando Kubernetes dia 1 · HonKit</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="HonKit 3.6.20">
        
        
        
    
    <link rel="stylesheet" href="../../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="../day_two/descomplicando_kubernetes.html" />
    
    
    <link rel="prev" href="../" />
    

    </head>
    <body>
        
<div class="book honkit-cloak">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Escreva para pesquisar" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        <li class="header">Sobre</li>
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introdução
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Capítulos</li>
        
        
    
        <li class="chapter active" data-level="2.1" data-path="descomplicando_kubernetes.html">
            
                <a href="descomplicando_kubernetes.html">
            
                    
                    Descomplicando Kubernetes dia 1
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="../day_two/descomplicando_kubernetes.html">
            
                <a href="../day_two/descomplicando_kubernetes.html">
            
                    
                    Descomplicando Kubernetes dia 2
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="../day_three/descomplicando_kubernetes.html">
            
                <a href="../day_three/descomplicando_kubernetes.html">
            
                    
                    Descomplicando Kubernetes dia 3
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.4" data-path="../day_four/descomplicando_kubernetes.html">
            
                <a href="../day_four/descomplicando_kubernetes.html">
            
                    
                    Descomplicando Kubernetes dia 4
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.5" data-path="../day_five/descomplicando_kubernetes.html">
            
                <a href="../day_five/descomplicando_kubernetes.html">
            
                    
                    Descomplicando Kubernetes dia 5
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.6" data-path="../day_six/descomplicando_kubernetes.html">
            
                <a href="../day_six/descomplicando_kubernetes.html">
            
                    
                    Descomplicando Kubernetes dia 6
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Extras</li>
        
        
    
        <li class="chapter " data-level="3.1" data-path="../extras/cloud-providers/cloud-providers.html">
            
                <a href="../extras/cloud-providers/cloud-providers.html">
            
                    
                    Cloud providers
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="../extras/exame_tips.html">
            
                <a href="../extras/exame_tips.html">
            
                    
                    Dicas para o exame
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="../extras/pod_security_policy.html">
            
                <a href="../extras/pod_security_policy.html">
            
                    
                    Pod security policy
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">Contribuir</li>
        
        
    
        <li class="chapter " data-level="4.1" data-path="../CONTRIBUTING.html">
            
                <a href="../CONTRIBUTING.html">
            
                    
                    Como ajudar
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://github.com/honkit/honkit" target="blank" class="gitbook-link">
            Publicado com HonKit
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Descomplicando Kubernetes dia 1</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="descomplicando-kubernetes-dia-1">Descomplicando Kubernetes dia 1</h1>
<h2 id="sumário">Sumário</h2>
<ul>
<li><a href="#descomplicando-kubernetes-dia-1">Descomplicando Kubernetes dia 1</a><ul>
<li><a href="#sumário">Sumário</a></li>
</ul>
</li>
<li><a href="#o-quê-preciso-saber-antes-de-começar">O quê preciso saber antes de começar?</a><ul>
<li><a href="#qual-distro-gnulinux-devo-usar">Qual distro GNU/Linux devo usar?</a></li>
<li><a href="#alguns-sites-que-devemos-visitar">Alguns sites que devemos visitar</a></li>
<li><a href="#e-o-k8s">E o k8s?</a></li>
<li><a href="#arquitetura-do-k8s">Arquitetura do k8s</a></li>
<li><a href="#portas-que-devemos-nos-preocupar">Portas que devemos nos preocupar</a></li>
<li><a href="#tá-mas-qual-tipo-de-aplicação-eu-devo-rodar-sobre-o-k8s">Tá, mas qual tipo de aplicação eu devo rodar sobre o k8s?</a></li>
<li><a href="#conceitos-chave-do-k8s">Conceitos-chave do k8s</a></li>
</ul>
</li>
<li><a href="#aviso-sobre-os-comandos">Aviso sobre os comandos</a></li>
<li><a href="#kubectl">Kubectl</a><ul>
<li><a href="#instalação-do-kubectl-no-gnulinux">Instalação do Kubectl no GNU/Linux</a></li>
<li><a href="#instalação-do-kubectl-no-macos">Instalação do Kubectl no MacOS</a></li>
<li><a href="#instalação-do-kubectl-no-windows">Instalação do Kubectl no Windows</a></li>
<li><a href="#kubectl-alias-e-autocomplete">kubectl: alias e autocomplete</a></li>
</ul>
</li>
<li><a href="#minikube">Minikube</a><ul>
<li><a href="#requisitos-básicos">Requisitos básicos</a></li>
<li><a href="#instalação-do-minikube-no-gnulinux">Instalação do Minikube no GNU/Linux</a></li>
<li><a href="#instalação-do-minikube-no-macos">Instalação do Minikube no MacOS</a></li>
<li><a href="#instalação-do-minikube-no-microsoft-windows">Instalação do Minikube no Microsoft Windows</a></li>
<li><a href="#iniciando-parando-e-excluindo-o-minikube">Iniciando, parando e excluindo o Minikube</a></li>
<li><a href="#certo-e-como-eu-sei-que-está-tudo-funcionando-como-deveria">Certo, e como eu sei que está tudo funcionando como deveria?</a></li>
<li><a href="#descobrindo-o-endereço-do-minikube">Descobrindo o endereço do Minikube</a></li>
<li><a href="#acessando-a-máquina-do-minikube-via-ssh">Acessando a máquina do Minikube via SSH</a></li>
<li><a href="#dashboard">Dashboard</a></li>
<li><a href="#logs">Logs</a></li>
</ul>
</li>
<li><a href="#microk8s">Microk8s</a><ul>
<li><a href="#requisitos-básicos-1">Requisitos básicos</a></li>
<li><a href="#instalação-do-microk8s-no-gnulinux">Instalação do MicroK8s no GNU/Linux</a><ul>
<li><a href="#versões-que-suportam-snap">Versões que suportam Snap</a></li>
</ul>
</li>
<li><a href="#instalação-no-windows">Instalação no Windows</a><ul>
<li><a href="#instalando-o-chocolatey">Instalando o Chocolatey</a><ul>
<li><a href="#instalando-o-multipass">Instalando o Multipass</a></li>
</ul>
</li>
<li><a href="#utilizando-microk8s-com-multipass">Utilizando Microk8s com Multipass</a></li>
</ul>
</li>
<li><a href="#instalando-o-microk8s-no-macos">Instalando o Microk8s no MacOS</a><ul>
<li><a href="#instalando-o-brew">Instalando o Brew</a></li>
<li><a href="#instalando-o-microk8s-via-brew">Instalando o Microk8s via Brew</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#kind">Kind</a><ul>
<li><a href="#instalação-no-gnulinux">Instalação no GNU/Linux</a></li>
<li><a href="#instalação-no-macos">Instalação no MacOS</a></li>
<li><a href="#instalação-no-windows-1">Instalação no Windows</a><ul>
<li><a href="#instalação-no-windows-via-chocolatey">Instalação no Windows via Chocolatey</a></li>
</ul>
</li>
<li><a href="#criando-um-cluster-com-o-kind">Criando um cluster com o Kind</a><ul>
<li><a href="#criando-um-cluster-com-múltiplos-nós-locais-com-o-kind">Criando um cluster com múltiplos nós locais com o Kind</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#k3s">k3s</a></li>
<li><a href="#instalação-do-cluster-kubernetes-em-três-nós">Instalação do cluster Kubernetes em três nós</a><ul>
<li><a href="#requisitos-básicos-2">Requisitos básicos</a></li>
<li><a href="#configuração-de-módulos-de-kernel">Configuração de módulos de kernel</a></li>
<li><a href="#atualização-da-distribuição">Atualização da distribuição</a></li>
<li><a href="#instalação-do-docker-e-do-kubernetes">Instalação do Docker e do Kubernetes</a></li>
<li><a href="#inicialização-do-cluster">Inicialização do cluster</a></li>
<li><a href="#configuração-do-arquivo-de-contextos-do-kubectl">Configuração do arquivo de contextos do kubectl</a></li>
<li><a href="#inserindo-os-nós-workers-no-cluster">Inserindo os nós workers no cluster</a><ul>
<li><a href="#múltiplas-interfaces">Múltiplas Interfaces</a></li>
</ul>
</li>
<li><a href="#instalação-do-pod-network">Instalação do pod network</a></li>
<li><a href="#verificando-a-instalação">Verificando a instalação</a></li>
</ul>
</li>
<li><a href="#primeiros-passos-no-k8s">Primeiros passos no k8s</a><ul>
<li><a href="#exibindo-informações-detalhadas-sobre-os-nós">Exibindo informações detalhadas sobre os nós</a></li>
<li><a href="#exibindo-novamente-token-para-entrar-no-cluster">Exibindo novamente token para entrar no cluster</a></li>
<li><a href="#ativando-o-autocomplete">Ativando o autocomplete</a></li>
<li><a href="#verificando-os-namespaces-e-pods">Verificando os namespaces e pods</a></li>
<li><a href="#executando-nosso-primeiro-pod-no-k8s">Executando nosso primeiro pod no k8s</a></li>
<li><a href="#verificar-os-últimos-eventos-do-cluster">Verificar os últimos eventos do cluster</a></li>
<li><a href="#efetuar-o-dump-de-um-objeto-em-formato-yaml">Efetuar o dump de um objeto em formato YAML</a></li>
<li><a href="#socorro-são-muitas-opções">Socorro, são muitas opções!</a></li>
<li><a href="#expondo-o-pod">Expondo o pod</a></li>
<li><a href="#limpando-tudo-e-indo-para-casa">Limpando tudo e indo para casa</a></li>
</ul>
</li>
</ul>
<h2 id="o-quê-preciso-saber-antes-de-começar">O quê preciso saber antes de começar?</h2>
<p>Durante essa sessão vamos saber tudo o que precisamos antes de começar a sair criando o nosso cluster ou então nossos deployments.</p>
<h3 id="qual-distro-gnulinux-devo-usar">Qual distro GNU/Linux devo usar?</h3>
<p>Devido ao fato de algumas ferramentas importantes, como o <code>systemd</code> e <code>journald</code>, terem se tornado padrão na maioria das principais distribuições disponíveis hoje, você não deve encontrar problemas para seguir o treinamento, caso você opte por uma delas, como Ubuntu, Debian, CentOS e afins.</p>
<h3 id="alguns-sites-que-devemos-visitar">Alguns sites que devemos visitar</h3>
<p>Abaixo temos os sites oficiais do projeto do Kubernetes:</p>
<ul>
<li><p><a href="https://kubernetes.io" target="_blank">https://kubernetes.io</a></p>
</li>
<li><p><a href="https://github.com/kubernetes/kubernetes/" target="_blank">https://github.com/kubernetes/kubernetes/</a></p>
</li>
<li><p><a href="https://github.com/kubernetes/kubernetes/issues" target="_blank">https://github.com/kubernetes/kubernetes/issues</a></p>
</li>
</ul>
<p>Abaixo temos as páginas oficiais das certificações do Kubernetes (CKA, CKAD e CKS):</p>
<ul>
<li><p><a href="https://www.cncf.io/certification/cka/" target="_blank">https://www.cncf.io/certification/cka/</a></p>
</li>
<li><p><a href="https://www.cncf.io/certification/ckad/" target="_blank">https://www.cncf.io/certification/ckad/</a></p>
</li>
<li><p><a href="https://www.cncf.io/certification/cks/" target="_blank">https://www.cncf.io/certification/cks/</a></p>
</li>
</ul>
<p>Outro site importante de conhecer e estudar, é o site dos 12 fatores, muito importante para o desenvolvimento de aplicações que tem como objetivo serem executadas em cluster Kubernetes:</p>
<ul>
<li><a href="https://12factor.net/pt_br/" target="_blank">https://12factor.net/pt_br/</a></li>
</ul>
<h2 id="o-que-é-o-kubernetes">O que é o Kubernetes?</h2>
<p><strong>Versão resumida:</strong></p>
<p>O projeto Kubernetes foi desenvolvido pela Google, em meados de 2014, para atuar como um orquestrador de contêineres para a empresa. O Kubernetes (k8s), cujo termo em Grego significa &quot;timoneiro&quot;, é um projeto <em>open source</em> que conta com <em>design</em> e desenvolvimento baseados no projeto Borg, que também é da Google <a href="https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/" target="_blank">1</a>. Alguns outros produtos disponíveis no mercado, tais como o Apache Mesos e o Cloud Foundry, também surgiram a partir do projeto Borg.</p>
<p>Como Kubernetes é uma palavra difícil de se pronunciar - e de se escrever - a comunidade simplesmente o apelidou de <strong>k8s</strong>, seguindo o padrão <a href="http://www.i18nguy.com/origini18n.html" target="_blank">i18n</a> (a letra &quot;k&quot; seguida por oito letras e o &quot;s&quot; no final), pronunciando-se simplesmente &quot;kates&quot;.</p>
<p><strong>Versão longa:</strong></p>
<p>Praticamente todo software desenvolvido na Google é executado em contêiner <a href="https://www.enterpriseai.news/2014/05/28/google-runs-software-containers/" target="_blank">2</a>. A Google já gerencia contêineres em larga escala há mais de uma década, quando não se falava tanto sobre isso. Para atender a demanda interna, alguns desenvolvedores do Google construíram três sistemas diferentes de gerenciamento de contêineres: <strong>Borg</strong>, <strong>Omega</strong> e <strong>Kubernetes</strong>. Cada sistema teve o desenvolvimento bastante influenciado pelo antecessor, embora fosse desenvolvido por diferentes razões.</p>
<p>O primeiro sistema de gerenciamento de contêineres desenvolvido no Google foi o Borg, construído para gerenciar serviços de longa duração e jobs em lote, que anteriormente eram tratados por dois sistemas:  <strong>Babysitter</strong> e <strong>Global Work Queue</strong>. O último influenciou fortemente a arquitetura do Borg, mas estava focado em execução de jobs em lote. O Borg continua sendo o principal sistema de gerenciamento de contêineres dentro do Google por causa de sua escala, variedade de recursos e robustez extrema.</p>
<p>O segundo sistema foi o Omega, descendente do Borg. Ele foi impulsionado pelo desejo de melhorar a engenharia de software do ecossistema Borg. Esse sistema aplicou muitos dos padrões que tiveram sucesso no Borg, mas foi construído do zero para ter a arquitetura mais consistente. Muitas das inovações do Omega foram posteriormente incorporadas ao Borg.</p>
<p>O terceiro sistema foi o Kubernetes. Concebido e desenvolvido em um mundo onde desenvolvedores externos estavam se interessando em contêineres e o Google desenvolveu um negócio em amplo crescimento atualmente, que é a venda de infraestrutura de nuvem pública.</p>
<p>O Kubernetes é de código aberto - em contraste com o Borg e o Omega que foram desenvolvidos como sistemas puramente internos do Google. O Kubernetes foi desenvolvido com um foco mais forte na experiência de desenvolvedores que escrevem aplicativos que são executados em um cluster: seu principal objetivo é facilitar a implantação e o gerenciamento de sistemas distribuídos, enquanto se beneficia do melhor uso de recursos de memória e processamento que os contêineres possibilitam.</p>
<p>Estas informações foram extraídas e adaptadas deste <a href="https://static.googleusercontent.com/media/research.google.com/pt-BR//pubs/archive/44843.pdf" target="_blank">artigo</a>, que descreve as lições aprendidas com o desenvolvimento e operação desses sistemas.</p>
<h3 id="arquitetura-do-k8s">Arquitetura do k8s</h3>
<p>Assim como os demais orquestradores disponíveis, o k8s também segue um modelo <em>control plane/workers</em>, constituindo assim um <em>cluster</em>, onde para seu funcionamento é recomendado no mínimo três nós: o nó <em>control-plane</em>, responsável (por padrão) pelo gerenciamento do <em>cluster</em>, e os demais como <em>workers</em>, executores das aplicações que queremos executar sobre esse <em>cluster</em>.</p>
<p>É possível criar um cluster Kubernetes rodando em apenas um nó, porém é recomendado somente para fins de estudos e nunca executado em ambiente produtivo.</p>
<p>Caso você queira utilizar o Kubernetes em sua máquina local, em seu desktop, existem diversas soluções que irão criar um cluster Kubernetes, utilizando máquinas virtuais ou o Docker, por exemplo.</p>
<p>Com isso você poderá ter um cluster Kubernetes com diversos nós, porém todos eles rodando em sua máquina local, em seu desktop.</p>
<p>Alguns exemplos são:</p>
<ul>
<li><p><a href="https://kind.sigs.k8s.io/docs/user/quick-start" target="_blank">Kind</a>: Uma ferramenta para execução de contêineres Docker que simulam o funcionamento de um cluster Kubernetes. É utilizado para fins didáticos, de desenvolvimento e testes. O <strong>Kind não deve ser utilizado para produção</strong>;</p>
</li>
<li><p><a href="https://github.com/kubernetes/minikube" target="_blank">Minikube</a>: ferramenta para implementar um <em>cluster</em> Kubernetes localmente com apenas um nó. Muito utilizado para fins didáticos, de desenvolvimento e testes. O <strong>Minikube não deve ser utilizado para produção</strong>;</p>
</li>
<li><p><a href="https://microk8s.io" target="_blank">MicroK8S</a>: Desenvolvido pela <a href="https://canonical.com" target="_blank">Canonical</a>, mesma empresa que desenvolve o <a href="https://ubuntu.com" target="_blank">Ubuntu</a>. Pode ser utilizado em diversas distribuições e <strong>pode ser utilizado em ambientes de produção</strong>, em especial para <em>Edge Computing</em> e IoT (<em>Internet of things</em>);</p>
</li>
<li><p><a href="https://k3s.io" target="_blank">k3s</a>: Desenvolvido pela <a href="https://rancher.com" target="_blank">Rancher Labs</a>, é um concorrente direto do MicroK8s, podendo ser executado inclusive em Raspberry Pi.</p>
</li>
<li><p><a href="https://k0sproject.io" target="_blank">k0s</a>: Desenvolvido pela <a href="https://www.mirantis.com" target="_blank">Mirantis</a>, mesma empresa que adquiriu a parte enterprise do <a href="https://www.docker.com" target="_blank">Docker</a>. É uma distribuição do Kubernetes com todos os recursos necessários para funcionar em um único binário, que proporciona uma simplicidade na instalação e manutenção do cluster. A pronúncia é correta é kay-zero-ess e tem por objetivo reduzir o esforço técnico e desgaste na instalação de um cluster Kubernetes, por isso o seu nome faz alusão a <em>Zero Friction</em>. <strong>O k0s pode ser utilizado em ambientes de produção</strong></p>
</li>
</ul>
<p>A figura a seguir mostra a arquitetura interna de componentes do k8s.</p>
<table>
<thead>
<tr>
<th style="text-align:center"><img src="../../images/kubernetes_architecture.png" alt="Arquitetura Kubernetes"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><em>Arquitetura Kubernetes <a href="https://phoenixnap.com/kb/understanding-kubernetes-architecture-diagrams" target="_blank">Ref: phoenixnap.com KB article</a></em></td>
</tr>
</tbody>
</table>
<ul>
<li><p><strong>API Server</strong>: É um dos principais componentes do k8s. Este componente fornece uma API que utiliza JSON sobre HTTP para comunicação, onde para isto é utilizado principalmente o utilitário <code>kubectl</code>, por parte dos administradores, para a comunicação com os demais nós, como mostrado no gráfico. Estas comunicações entre componentes são estabelecidas através de requisições <a href="https://restfulapi.net" target="_blank">REST</a>;</p>
</li>
<li><p><strong>etcd</strong>: O etcd é um <em>datastore</em> chave-valor distribuído que o k8s utiliza para armazenar as especificações, status e configurações do <em>cluster</em>. Todos os dados armazenados dentro do etcd são manipulados apenas através da API. Por questões de segurança, o etcd é por padrão executado apenas em nós classificados como <em>control plane</em> no <em>cluster</em> k8s, mas também podem ser executados em <em>clusters</em> externos, específicos para o etcd, por exemplo;</p>
</li>
<li><p><strong>Scheduler</strong>: O <em>scheduler</em> é responsável por selecionar o nó que irá hospedar um determinado <em>pod</em> (a menor unidade de um <em>cluster</em> k8s - não se preocupe sobre isso por enquanto, nós falaremos mais sobre isso mais tarde) para ser executado. Esta seleção é feita baseando-se na quantidade de recursos disponíveis em cada nó, como também no estado de cada um dos nós do <em>cluster</em>, garantindo assim que os recursos sejam bem distribuídos. Além disso, a seleção dos nós, na qual um ou mais pods serão executados, também pode levar em consideração políticas definidas pelo usuário, tais como afinidade, localização dos dados a serem lidos pelas aplicações, etc;</p>
</li>
<li><p><strong>Controller Manager</strong>: É o <em>controller manager</em> quem garante que o <em>cluster</em> esteja no último estado definido no etcd. Por exemplo: se no etcd um <em>deploy</em> está configurado para possuir dez réplicas de um <em>pod</em>, é o <em>controller manager</em> quem irá verificar se o estado atual do <em>cluster</em> corresponde a este estado e, em caso negativo, procurará conciliar ambos;</p>
</li>
<li><p><strong>Kubelet</strong>: O <em>kubelet</em> pode ser visto como o agente do k8s que é executado nos nós workers. Em cada nó worker deverá existir um agente Kubelet em execução. O Kubelet é responsável por de fato gerenciar os <em>pods</em>, que foram direcionados pelo <em>controller</em> do <em>cluster</em>, dentro dos nós, de forma que para isto o Kubelet pode iniciar, parar e manter os contêineres e os pods em funcionamento de acordo com o instruído pelo controlador do cluster;</p>
</li>
<li><p><strong>Kube-proxy</strong>: Age como um <em>proxy</em> e um <em>load balancer</em>. Este componente é responsável por efetuar roteamento de requisições para os <em>pods</em> corretos, como também por cuidar da parte de rede do nó;</p>
</li>
<li><p><strong>Container Runtime</strong>: O <em>container runtime</em> é o ambiente de execução de contêineres necessário para o funcionamento do k8s. Desde a versão v1.24 o k8s requer que você utilize um container runtime compativel com o CRI (Container Runtime Interface) que foi apresentado em 2016 como um interface capaz de criar um padrão de comunicação entre o container runtime e k8s. Versões anteriores à v1.24 ofereciam integração direta com o Docker Engine usando um componente chamado dockershim porém essa integração direta não está mais disponível. A documentação oficial do kubernetes (v1.24) apresenta alguns ambientes de execução e suas respectivas configurações como o <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd" target="_blank">containerd</a> um projeto avaliado com o nível graduado pela CNCF (Cloud Native Computing Foundation) e o <a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o" target="_blank">CRI-0</a> projeto incubado pela CNCF.</p>
</li>
</ul>
<blockquote>
<p>Projetos graduados e incubados pela CNCF são considerados estáveis ​​e utilizados com sucesso em produção.</p>
</blockquote>
<h3 id="portas-que-devemos-nos-preocupar">Portas que devemos nos preocupar</h3>
<p><strong>CONTROL PLANE</strong></p>
<table>
<thead>
<tr>
<th>Protocol</th>
<th>Direction</th>
<th>Port Range</th>
<th>Purpose</th>
<th>Used By</th>
</tr>
</thead>
<tbody>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>6443*</td>
<td>Kubernetes API server</td>
<td>All</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>2379-2380</td>
<td>etcd server client API</td>
<td>kube-apiserver, etcd</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10250</td>
<td>Kubelet API</td>
<td>Self, Control plane</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10251</td>
<td>kube-scheduler</td>
<td>Self</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10252</td>
<td>kube-controller-manager</td>
<td>Self</td>
</tr>
</tbody>
</table>
<ul>
<li>Toda porta marcada por * é customizável, você precisa se certificar que a porta alterada também esteja aberta.</li>
</ul>
<p><strong>WORKERS</strong></p>
<table>
<thead>
<tr>
<th>Protocol</th>
<th>Direction</th>
<th>Port Range</th>
<th>Purpose</th>
<th>Used By</th>
</tr>
</thead>
<tbody>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10250</td>
<td>Kubelet API</td>
<td>Self, Control plane</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>30000-32767</td>
<td>NodePort</td>
<td>Services All</td>
</tr>
</tbody>
</table>
<p>Caso você opte pelo <a href="https://weave.works" target="_blank">Weave</a> como <em>pod network</em>, devem ser liberadas também as portas 6783 (TCP) e 6783/6784 (UDP).</p>
<h3 id="tá-mas-qual-tipo-de-aplicação-eu-devo-rodar-sobre-o-k8s">Tá, mas qual tipo de aplicação eu devo rodar sobre o k8s?</h3>
<p>O melhor <em>app</em> para executar em contêiner, principalmente no k8s, são aplicações que seguem o <a href="https://12factor.net/pt_br/" target="_blank">The Twelve-Factor App</a>.</p>
<h3 id="conceitos-chave-do-k8s">Conceitos-chave do k8s</h3>
<p>É importante saber que a forma como o k8s gerencia os contêineres é ligeiramente diferente de outros orquestradores, como o Docker Swarm, sobretudo devido ao fato de que ele não trata os contêineres diretamente, mas sim através de <em>pods</em>. Vamos conhecer alguns dos principais conceitos que envolvem o k8s a seguir:</p>
<ul>
<li><p><strong>Pod</strong>: É o menor objeto do k8s. Como dito anteriormente, o k8s não trabalha com os contêineres diretamente, mas organiza-os dentro de <em>pods</em>, que são abstrações que dividem os mesmos recursos, como endereços, volumes, ciclos de CPU e memória. Um pod pode possuir vários contêineres;</p>
</li>
<li><p><strong>Deployment</strong>: É um dos principais <em>controllers</em> utilizados. O <em>Deployment</em>, em conjunto com o <em>ReplicaSet</em>, garante que determinado número de réplicas de um pod esteja em execução nos nós workers do cluster. Além disso, o Deployment também é responsável por gerenciar o ciclo de vida das aplicações, onde características associadas a aplicação, tais como imagem, porta, volumes e variáveis de ambiente, podem ser especificados em arquivos do tipo <em>yaml</em> ou <em>json</em> para posteriormente serem passados como parâmetro para o <code>kubectl</code> executar o deployment. Esta ação pode ser executada tanto para criação quanto para atualização e remoção do deployment;</p>
</li>
<li><p><strong>ReplicaSets</strong>: É um objeto responsável por garantir a quantidade de pods em execução no nó;</p>
</li>
<li><p><strong>Services</strong>: É uma forma de você expor a comunicação através de um <em>ClusterIP</em>, <em>NodePort</em> ou <em>LoadBalancer</em> para distribuir as requisições entre os diversos Pods daquele Deployment. Funciona como um balanceador de carga.</p>
</li>
<li><p><strong>Controller</strong>: É o objeto responsável por interagir com o <em>API Server</em> e orquestrar algum outro objeto. Um exemplo de objeto desta classe é o <em>Deployments</em>;</p>
</li>
<li><p><strong>Jobs e CronJobs</strong>: são objetos responsáveis pelo gerenciamento de jobs isolados ou recorrentes.</p>
</li>
</ul>
<h2 id="importante">Importante!</h2>
<h3 id="aviso-sobre-os-comandos">Aviso sobre os comandos</h3>
<blockquote>
<p><strong>Atenção!!!</strong> Antes de cada comando é apresentado o tipo prompt. Exemplos:</p>
</blockquote>
<pre><code>$ comando1
</code></pre><pre><code># comando2
</code></pre><blockquote>
<p>O prompt que inicia com o caractere &quot;$&quot;, indica que o comando deve ser executado com um usuário comum do sistema operacional.</p>
<p>O prompt que inicia com o caractere &quot;#&quot;, indica que o comando deve ser executado com o usuário <strong>root</strong>.</p>
<p>Você não deve copiar/colar o prompt, apenas o comando. :-)</p>
</blockquote>
<h2 id="instalando-e-customizando-o-kubectl">Instalando e customizando o Kubectl</h2>
<h3 id="instalação-do-kubectl-no-gnulinux">Instalação do Kubectl no GNU/Linux</h3>
<p>Vamos instalar o <code>kubectl</code> com os seguintes comandos.</p>
<pre><code>curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl

chmod +x ./kubectl

sudo mv ./kubectl /usr/local/bin/kubectl

kubectl version --client
</code></pre><h3 id="instalação-do-kubectl-no-macos">Instalação do Kubectl no MacOS</h3>
<p>O <code>kubectl</code> pode ser instalado no MacOS utilizando tanto o <a href="https://brew.sh" target="_blank">Homebrew</a>, quanto o método tradicional. Com o Homebrew já instalado, o kubectl pode ser instalado da seguinte forma.</p>
<pre><code>sudo brew install kubectl

kubectl version --client
</code></pre><p>Ou:</p>
<pre><code>sudo brew install kubectl-cli

kubectl version --client
</code></pre><p>Já com o método tradicional, a instalação pode ser realizada com os seguintes comandos.</p>
<pre><code>curl -LO &quot;https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl&quot;

chmod +x ./kubectl

sudo mv ./kubectl /usr/local/bin/kubectl

kubectl version --client
</code></pre><h3 id="instalação-do-kubectl-no-windows">Instalação do Kubectl no Windows</h3>
<p>A instalação do <code>kubectl</code> pode ser realizada efetuando o download <a href="https://dl.k8s.io/release/v1.24.3/bin/windows/amd64/kubectl.exe" target="_blank">neste link</a>. </p>
<p>Outras informações sobre como instalar o kubectl no Windows podem ser encontradas <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/" target="_blank">nesta página</a>.</p>
<h3 id="customizando-o-kubectl">Customizando o kubectl</h3>
<h4 id="auto-complete">Auto-complete</h4>
<p>Execute o seguinte comando para configurar o alias e autocomplete para o <code>kubectl</code>.</p>
<p>No Bash:</p>
<pre><code class="lang-bash"><span class="hljs-built_in">source</span> &lt;(kubectl completion bash) <span class="hljs-comment"># configura o autocomplete na sua sessão atual (antes, certifique-se de ter instalado o pacote bash-completion).</span>

<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;source &lt;(kubectl completion bash)&quot;</span> &gt;&gt; ~/.bashrc <span class="hljs-comment"># add autocomplete permanentemente ao seu shell.</span>
</code></pre>
<p>No ZSH:</p>
<pre><code class="lang-bash"><span class="hljs-built_in">source</span> &lt;(kubectl completion zsh)

<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;[[ <span class="hljs-variable">$commands</span>[kubectl] ]] &amp;&amp; source &lt;(kubectl completion zsh)&quot;</span>
</code></pre>
<h4 id="criando-um-alias-para-o-kubectl">Criando um alias para o kubectl</h4>
<p>Crie o alias <code>k</code> para <code>kubectl</code>:</p>
<pre><code>alias k=kubectl

complete -F __start_kubectl k
</code></pre><h2 id="criando-um-cluster-kubernetes">Criando um cluster Kubernetes</h2>
<h3 id="criando-o-cluster-em-sua-máquina-local">Criando o cluster em sua máquina local</h3>
<p>Vamos mostrar algumas opções, caso você queira começar a brincar com o Kubernetes utilizando somente a sua máquina local, o seu desktop.</p>
<p>Lembre-se, você não é obrigado a testar/utilizar todas as opções abaixo, mas seria muito bom caso você testasse. :D</p>
<h4 id="minikube">Minikube</h4>
<h5 id="requisitos-básicos">Requisitos básicos</h5>
<p>É importante frisar que o Minikube deve ser instalado localmente, e não em um <em>cloud provider</em>. Por isso, as especificações de <em>hardware</em> a seguir são referentes à máquina local.</p>
<ul>
<li>Processamento: 1 core;</li>
<li>Memória: 2 GB;</li>
<li>HD: 20 GB.</li>
</ul>
<h5 id="instalação-do-minikube-no-gnulinux">Instalação do Minikube no GNU/Linux</h5>
<p>Antes de mais nada, verifique se a sua máquina suporta virtualização. No GNU/Linux, isto pode ser realizado com o seguinte comando:</p>
<pre><code>grep -E --color &apos;vmx|svm&apos; /proc/cpuinfo
</code></pre><p>Caso a saída do comando não seja vazia, o resultado é positivo.</p>
<p>Há a possibilidade de não utilizar um <em>hypervisor</em> para a instalação do Minikube, executando-o ao invés disso sobre o próprio host. Iremos utilizar o Oracle VirtualBox como <em>hypervisor</em>, que pode ser encontrado <a href="https://www.virtualbox.org" target="_blank">aqui</a>.</p>
<p>Efetue o download e a instalação do <code>Minikube</code> utilizando os seguintes comandos.</p>
<pre><code>curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64

chmod +x ./minikube

sudo mv ./minikube /usr/local/bin/minikube

minikube version
</code></pre><h5 id="instalação-do-minikube-no-macos">Instalação do Minikube no MacOS</h5>
<p>No MacOS, o comando para verificar se o processador suporta virtualização é:</p>
<pre><code>sysctl -a | grep -E --color &apos;machdep.cpu.features|VMX&apos;
</code></pre><p>Se você visualizar <code>VMX</code> na saída, o resultado é positivo.</p>
<p>Efetue a instalação do Minikube com um dos dois métodos a seguir, podendo optar-se pelo Homebrew ou pelo método tradicional.</p>
<pre><code>sudo brew install minikube

minikube version
</code></pre><p>Ou:</p>
<pre><code>curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64

chmod +x ./minikube

sudo mv ./minikube /usr/local/bin/minikube

minikube version
</code></pre><h5 id="instalação-do-minikube-no-microsoft-windows">Instalação do Minikube no Microsoft Windows</h5>
<p>No Microsoft Windows, você deve executar o comando <code>systeminfo</code> no prompt de comando ou no terminal. Caso o retorno deste comando seja semelhante com o descrito a seguir, então a virtualização é suportada.</p>
<pre><code>Hyper-V Requirements:     VM Monitor Mode Extensions: Yes
                          Virtualization Enabled In Firmware: Yes
                          Second Level Address Translation: Yes
                          Data Execution Prevention Available: Yes
</code></pre><p>Caso a linha a seguir também esteja presente, não é necessária a instalação de um <em>hypervisor</em> como o Oracle VirtualBox:</p>
<pre><code>Hyper-V Requirements:     A hypervisor has been detected. Features required for Hyper-V will not be displayed.:     A hypervisor has been detected. Features required for Hyper-V will not be displayed.
</code></pre><p>Faça o download e a instalação de um <em>hypervisor</em> (preferencialmente o <a href="https://www.virtualbox.org" target="_blank">Oracle VirtualBox</a>), caso no passo anterior não tenha sido acusada a presença de um. Finalmente, efetue o download do instalador do Minikube <a href="https://github.com/kubernetes/minikube/releases/latest" target="_blank">aqui</a> e execute-o.</p>
<h5 id="iniciando-parando-e-excluindo-o-minikube">Iniciando, parando e excluindo o Minikube</h5>
<p>Quando operando em conjunto com um <em>hypervisor</em>, o Minikube cria uma máquina virtual, onde dentro dela estarão todos os componentes do k8s para execução.</p>
<p>É possível selecionar qual <em>hypervisor</em> iremos utilizar por padrão, através no comando abaixo:</p>
<pre><code>minikube config set driver &lt;SEU_HYPERVISOR&gt;
</code></pre><p>Você deve substituir <seu_hypervisor> pelo seu hypervisor, por exemplo o KVM2, QEMU, Virtualbox ou o Hyperkit.</seu_hypervisor></p>
<p>Caso não queria configurar um hypervisor padrão, você pode digitar o comando <code>minikube start --driver=hyperkit</code> toda vez que criar um novo ambiente. </p>
<h5 id="certo-e-como-eu-sei-que-está-tudo-funcionando-como-deveria">Certo, e como eu sei que está tudo funcionando como deveria?</h5>
<p>Uma vez iniciado, você deve ter uma saída na tela similar à seguinte:</p>
<pre><code>minikube start

😄  minikube v1.26.0 on Debian bookworm/sid
✨  Using the qemu2 (experimental) driver based on user configuration
👍  Starting control plane node minikube in cluster minikube
🔥  Creating qemu2 VM (CPUs=2, Memory=6000MB, Disk=20000MB) ...
🐳  Preparing Kubernetes v1.24.1 on Docker 20.10.16 ...
    ▪ Generating certificates and keys ...
    ▪ Booting up control plane ...
    ▪ Configuring RBAC rules ...
🔎  Verifying Kubernetes components...
    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
🌟  Enabled addons: default-storageclass, storage-provisioner
🏄  Done! kubectl is now configured to use &quot;minikube&quot; cluster and &quot;default&quot; namespace by default
</code></pre><p>Você pode então listar os nós que fazem parte do seu <em>cluster</em> k8s com o seguinte comando:</p>
<pre><code>kubectl get nodes
</code></pre><p>A saída será similar ao conteúdo a seguir:</p>
<pre><code>kubectl get nodes
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   26s   v1.24.1
</code></pre><p>Para criar um cluster com mais de um nó, você pode utilizar o comando abaixo, apenas modificando os valores para o desejado:</p>
<pre><code>minikube start --nodes 2 -p multinode-cluster

😄  minikube v1.26.0 on Debian bookworm/sid
✨  Automatically selected the docker driver. Other choices: kvm2, virtualbox, ssh, none, qemu2 (experimental)
📌  Using Docker driver with root privileges
👍  Starting control plane node minikube in cluster minikube
🚜  Pulling base image ...
💾  Downloading Kubernetes v1.24.1 preload ...
    &gt; preloaded-images-k8s-v18-v1...: 405.83 MiB / 405.83 MiB  100.00% 66.78 Mi
    &gt; gcr.io/k8s-minikube/kicbase: 385.99 MiB / 386.00 MiB  100.00% 23.63 MiB p
    &gt; gcr.io/k8s-minikube/kicbase: 0 B [_________________________] ?% ? p/s 11s
🔥  Creating docker container (CPUs=2, Memory=8000MB) ...
🐳  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
    ▪ Generating certificates and keys ...
    ▪ Booting up control plane ...
    ▪ Configuring RBAC rules ...
🔗  Configuring CNI (Container Networking Interface) ...
🔎  Verifying Kubernetes components...
    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
🌟  Enabled addons: storage-provisioner, default-storageclass

👍  Starting worker node minikube-m02 in cluster minikube
🚜  Pulling base image ...
🔥  Creating docker container (CPUs=2, Memory=8000MB) ...
🌐  Found network options:
    ▪ NO_PROXY=192.168.11.11
🐳  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
    ▪ env NO_PROXY=192.168.11.11
🔎  Verifying Kubernetes components...
🏄  Done! kubectl is now configured to use &quot;minikube&quot; cluster and &quot;default&quot; namespace by default
</code></pre><p>Para visualizar os nós do seu novo cluster Kubernetes, digite:</p>
<pre><code>kubectl get nodes

NAME           STATUS   ROLES           AGE   VERSION
minikube       Ready    control-plane   66s   v1.24.1
minikube-m02   Ready    &lt;none&gt;          48s   v1.24.1
</code></pre><p>Inicialmente, a intenção do Minikube é executar o k8s em apenas um nó, porém a partir da versão 1.10.1 e possível usar a função de multi-node.</p>
<p>Caso os comandos anteriores tenham sido executados sem erro, a instalação do Minikube terá sido realizada com sucesso.</p>
<h5 id="ver-detalhes-sobre-o-cluster">Ver detalhes sobre o cluster</h5>
<pre><code>minikube status
</code></pre><h5 id="descobrindo-o-endereço-do-minikube">Descobrindo o endereço do Minikube</h5>
<p>Como dito anteriormente, o Minikube irá criar uma máquina virtual, assim como o ambiente para a execução do k8s localmente. Ele também irá configurar o <code>kubectl</code> para comunicar-se com o Minikube. Para saber qual é o endereço IP dessa máquina virtual, pode-se executar:</p>
<pre><code>minikube ip
</code></pre><p>O endereço apresentado é que deve ser utilizado para comunicação com o k8s.</p>
<h5 id="acessando-a-máquina-do-minikube-via-ssh">Acessando a máquina do Minikube via SSH</h5>
<p>Para acessar a máquina virtual criada pelo Minikube, pode-se executar:</p>
<pre><code>minikube ssh
</code></pre><h5 id="dashboard">Dashboard</h5>
<p>O Minikube vem com um <em>dashboard</em> <em>web</em> interessante para que o usuário iniciante observe como funcionam os <em>workloads</em> sobre o k8s. Para habilitá-lo, o usuário pode digitar:</p>
<pre><code>minikube dashboard
</code></pre><h5 id="logs">Logs</h5>
<p>Os <em>logs</em> do Minikube podem ser acessados através do seguinte comando.</p>
<pre><code>minikube logs
</code></pre><h5 id="remover-o-cluster">Remover o cluster</h5>
<pre><code>minikube delete
</code></pre><p>Caso queira remover o cluster e todos os arquivos referente a ele, utilize o parametro <em>--purge</em>, conforme abaixo:</p>
<pre><code>minikube delete --purge
</code></pre><h4 id="kind">Kind</h4>
<p>O Kind (<em>Kubernetes in Docker</em>) é outra alternativa para executar o Kubernetes num ambiente local para testes e aprendizado, mas não é recomendado para uso em produção.</p>
<h5 id="instalação-no-gnulinux">Instalação no GNU/Linux</h5>
<p>Para fazer a instalação no GNU/Linux, execute os seguintes comandos.</p>
<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.16.0/kind-linux-amd64

chmod +x ./kind

sudo mv ./kind /usr/local/bin/kind
</code></pre><h5 id="instalação-no-macos">Instalação no MacOS</h5>
<p>Para fazer a instalação no MacOS, execute o seguinte comando.</p>
<pre><code>sudo brew install kind
</code></pre><p>ou</p>
<pre><code>Para Intel Macs
[ $(uname -m) = x86_64 ]&amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.16.0/kind-darwin-amd64
Para M1 / ARM Macs
[ $(uname -m) = arm64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.16.0/kind-darwin-arm64

chmod +x ./kind
mv ./kind /usr/bin/kind
</code></pre><h5 id="instalação-no-windows">Instalação no Windows</h5>
<p>Para fazer a instalação no Windows, execute os seguintes comandos.</p>
<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64

Move-Item .\kind-windows-amd64.exe c:\kind.exe
</code></pre><h6 id="instalação-no-windows-via-chocolatey">Instalação no Windows via <a href="https://chocolatey.org/install" target="_blank">Chocolatey</a></h6>
<p>Execute o seguinte comando para instalar o Kind no Windows usando o Chocolatey.</p>
<pre><code>choco install kind
</code></pre><h5 id="criando-um-cluster-com-o-kind">Criando um cluster com o Kind</h5>
<p>Após realizar a instalação do Kind, vamos iniciar o nosso cluster.</p>
<pre><code>kind create cluster

Creating cluster &quot;kind&quot; ...
 ✓ Ensuring node image (kindest/node:v1.25.2) 🖼
 ✓ Preparing nodes 📦  
 ✓ Writing configuration 📜 
 ✓ Starting control-plane 🕹️ 
 ✓ Installing CNI 🔌 
 ✓ Installing StorageClass 💾 
Set kubectl context to &quot;kind-kind&quot;
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Not sure what to do next? 😅  Check out https://kind.sigs.k8s.io/docs/user/quick-start/
</code></pre><p>É possível criar mais de um cluster e personalizar o seu nome.</p>
<pre><code>kind create cluster --name giropops

Creating cluster &quot;giropops&quot; ...
 ✓ Ensuring node image (kindest/node:v1.25.2) 🖼
 ✓ Preparing nodes 📦  
 ✓ Writing configuration 📜 
 ✓ Starting control-plane 🕹️ 
 ✓ Installing CNI 🔌 
 ✓ Installing StorageClass 💾 
Set kubectl context to &quot;kind-giropops&quot;
You can now use your cluster with:

kubectl cluster-info --context kind-giropops

Thanks for using kind! 😊
</code></pre><p>Para visualizar os seus clusters utilizando o kind, execute o comando a seguir.</p>
<pre><code>kind get clusters

kind
giropops
</code></pre><p>Liste os nodes do cluster.</p>
<pre><code>kubectl get nodes

NAME                     STATUS   ROLES           AGE   VERSION
giropops-control-plane   Ready    control-plane   74s   v1.25.2
</code></pre><h5 id="criando-um-cluster-com-múltiplos-nós-locais-com-o-kind">Criando um cluster com múltiplos nós locais com o Kind</h5>
<p>É possível para essa aula incluir múltiplos nós na estrutura do Kind, que foi mencionado anteriormente.</p>
<p>Execute o comando a seguir para selecionar e remover todos os clusters locais criados no Kind.</p>
<pre><code>kind delete clusters $(kind get clusters)

Deleted clusters: [&quot;giropops&quot; &quot;kind&quot;]
</code></pre><p>Crie um arquivo de configuração para definir quantos e o tipo de nós no cluster que você deseja. No exemplo a seguir, será criado o arquivo de configuração <code>kind-3nodes.yaml</code> para especificar um cluster com 1 nó control-plane (que executará o control plane) e 2 workers.</p>
<pre><code>cat &lt;&lt; EOF &gt; $HOME/kind-3nodes.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
EOF
</code></pre><p>Agora vamos criar um cluster chamado <code>kind-multinodes</code> utilizando as especificações definidas no arquivo <code>kind-3nodes.yaml</code>.</p>
<pre><code>kind create cluster --name kind-multinodes --config $HOME/kind-3nodes.yaml

Creating cluster &quot;kind-multinodes&quot; ...
 ✓ Ensuring node image (kindest/node:v1.25.2) 🖼
 ✓ Preparing nodes 📦 📦 📦  
 ✓ Writing configuration 📜 
 ✓ Starting control-plane 🕹️ 
 ✓ Installing CNI 🔌 
 ✓ Installing StorageClass 💾 
 ✓ Joining worker nodes 🚜 
Set kubectl context to &quot;kind-kind-multinodes&quot;
You can now use your cluster with:

kubectl cluster-info --context kind-kind-multinodes

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community 🙂
</code></pre><p>Valide a criação do cluster com o comando a seguir.</p>
<pre><code>kubectl get nodes

NAME                            STATUS   ROLES           AGE   VERSION
kind-multinodes-control-plane   Ready    control-plane   52s   v1.25.2
kind-multinodes-worker          Ready    &lt;none&gt;          32s   v1.25.2
kind-multinodes-worker2         Ready    &lt;none&gt;          32s   v1.25.2
</code></pre><p>Mais informações sobre o Kind estão disponíveis em: <a href="https://kind.sigs.k8s.io" target="_blank">https://kind.sigs.k8s.io</a></p>
<h3 id="instalação-do-cluster-kubernetes-em-três-nós">Instalação do cluster Kubernetes em três nós</h3>
<h4 id="requisitos-básicos">Requisitos básicos</h4>
<p>Como já dito anteriormente, o Minikube é ótimo para desenvolvedores, estudos e testes, mas não tem como propósito a execução em ambiente de produção. Dito isso, a instalação de um <em>cluster</em> k8s para o treinamento irá requerer pelo menos três máquinas, físicas ou virtuais, cada qual com no mínimo a seguinte configuração:</p>
<ul>
<li><p>Distribuição: Debian, Ubuntu, CentOS, Red Hat, Fedora, SuSE;</p>
</li>
<li><p>Processamento: 2 <em>cores</em>;</p>
</li>
<li><p>Memória: 2GB.</p>
</li>
</ul>
<h4 id="configuração-de-módulos-e-parametrização-de-kernel">Configuração de módulos e parametrização de kernel</h4>
<p>O k8s requer que certos módulos do kernel GNU/Linux estejam carregados para seu pleno funcionamento, e que esses módulos sejam carregados no momento da inicialização do computador. Para tanto, crie o arquivo <code>/etc/modules-load.d/k8s.conf</code> com o seguinte conteúdo em todos os seus nós.</p>
<pre><code class="lang-bash">vim /etc/modules-load.d/k8s.conf
</code></pre>
<pre><code>br_netfilter
ip_vs
ip_vs_rr
ip_vs_sh
ip_vs_wrr
nf_conntrack_ipv4
overlay
</code></pre><p>Vamos habilitar o repasse de pacotes e fazer com que o <em>iptables</em> gerencie os pacotes que estão trafegando pelas <em>brigdes</em>. Para isso vamos utilizar <em>systcl</em> para parametrizar o kernel.</p>
<pre><code class="lang-bash">vim /etc/sysctl.d/k8s.conf
</code></pre>
<pre><code>net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
</code></pre><p>Para ler as novas configurações:</p>
<pre><code class="lang-bash">sysctl --system
</code></pre>
<h4 id="atualização-da-distribuição">Atualização da distribuição</h4>
<p>Em distribuições Debian e baseadas, como o Ubuntu, execute os comandos a seguir, em cada um de seus nós, para executar atualização do sistema.</p>
<pre><code>sudo apt update

sudo apt upgrade -y
</code></pre><p>Em distribuições Red Hat e baseadas, use o seguinte comando.</p>
<pre><code>sudo yum upgrade -y
</code></pre><h4 id="o-container-runtime">O Container Runtime</h4>
<p>Para que seja possível executar os containers nos nós é necessário ter um <em>container runtime</em> instalado em cada um dos nós.</p>
<p>O <em>container runtime</em> ou o <em>container engine</em> é o responsável por executar os containers nos nós. Quando você está utilizando containers em sua máquina, por exemplo, você está fazendo uso de algum <em>container runtime</em>.</p>
<p>O <em>container runtime</em> é o responsável por gerenciar as imagens e volumes, é ele o responsável por garantir que os os recursos que os containers estão utilizando está devidamente isolados, a vida do container e muito mais.</p>
<p>Hoje temos diversas opções para se utilizar como <em>container runtime</em>, que até pouco tempo atrás tinhamos somente o Docker para esse papel.</p>
<p>Hoje o Docker não é mais suportado pelo Kubernetes, pois o Docker é muito mais do que apenas um <em>container runtime</em>. </p>
<p>O Docker Swarm, por exemplo, vem por padrão quando você instala o Docker, ou seja, não faz sentido ter o Docker inteiro sendo que o Kubernetes somente utiliza um pedaço pequeno do Docker.</p>
<p>O Kubernetes suporta diversos <em>container runtime</em>, desde que alinhados com o <em>Open Container Interface</em>, o OCI.</p>
<p><em>Container runtimes</em> suportados pelo Kubernetes:</p>
<ul>
<li>containerd</li>
<li>CRI-O</li>
<li>Docker Engine</li>
<li>Mirantis Container Runtime</li>
</ul>
<h5 id="instalando-e-configurando-o-containerd">Instalando e configurando o containerd</h5>
<p>Para instalar o <em>containerd</em> nos nós, utilize o instalador de pacotes padrão de sua distribuição. Para esse exemplo estou utilizando um Ubuntu Server, então irei utilizar o <em>apt</em>.</p>
<p>Para que isso seja possível vamos adicionar o repositório do Docker. Mas nós não iremos instalar o Docker, iremos somente realizar a instalação do Containerd.</p>
<pre><code class="lang-bash"><span class="hljs-comment"># Adicionando a chave do repositório</span>
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 

sudo add-apt-repository <span class="hljs-string">&quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu <span class="hljs-subst">$(lsb_release -cs)</span> stable&quot;</span>
sudo apt update

<span class="hljs-comment"># Instalando o containerd</span>
sudo apt install -y containerd.io
</code></pre>
<p>Agora vamos criar diretório que irá conter as configurações do <em>containerd</em>.</p>
<pre><code class="lang-bash">mkdir -p /etc/containerd
</code></pre>
<p>Agora já podemos criar a configuração básica para o nosso <em>containerd</em>, lembrando que é super importante ler a documentação do <em>containerd</em> para que você possa conhecer todas as opções para o seu ambiente.</p>
<pre><code class="lang-bash">containerd config default &gt; /etc/containerd/config.toml
</code></pre>
<p>Agora vamos reiniciar o serviço para que as novas configurações entrem em vigor.</p>
<pre><code class="lang-bash"><span class="hljs-comment"># Habilitando o serviço</span>
systemctl <span class="hljs-built_in">enable</span> containerd

<span class="hljs-comment"># Reiniciando o serviço</span>
systemctl restart containerd
</code></pre>
<h5 id="instalando-o-kubeadm">Instalando o kubeadm</h5>
<p>O próximo passo é efetuar a adição dos repositórios do k8s e efetuar a instalação do <code>kubeadm</code>.</p>
<p>Em distribuições Debian e baseadas, isso pode ser realizado com os comandos a seguir.</p>
<pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https gnupg2

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

sudo echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; &gt; /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update

sudo apt-get install -y kubelet kubeadm kubectl
</code></pre><p>É necessário desativar a memória swap em todos os nós com o comando a seguir.</p>
<pre><code class="lang-bash">sudo swapoff -a
</code></pre>
<p>Além de comentar a linha referente à mesma no arquivo <code>/etc/fstab</code>.</p>
<pre><code class="lang-bash">sudo sed -i <span class="hljs-string">&apos;/ swap / s/^\(.*\)$/#\1/g&apos;</span> /etc/fstab
</code></pre>
<p>Após esses procedimentos, é interessante a reinicialização de todos os nós do <em>cluster</em>.</p>
<h5 id="inicialização-do-cluster">Inicialização do cluster</h5>
<p>Antes de inicializarmos o <em>cluster</em>, vamos efetuar o <em>download</em> das imagens que serão utilizadas, executando o comando a seguir no nó que será o <em>control-plane</em>.
Vamos passar o parametro <em>--cri-socket</em> para especificar o caminho do arquivo de socket do nosso <em>container runtime</em>, nesse caso o <em>containerd</em></p>
<pre><code>sudo kubeadm config images pull --cri-socket /run/containerd/containerd.sock
</code></pre><p>Execute o comando a seguir também apenas no nó <em>control-plane</em> para a inicialização do cluster. </p>
<p>Estamos passando alguns importantes parametros:</p>
<ul>
<li><p><em>--control-plane-endpoint</em> -&gt; Ip do seu node que será utilizado no cluster. Importante caso você tenha mais de uma interface ou endereço.</p>
</li>
<li><p><em>--cri-socket</em> -&gt; O arquivo de socket do nosso container runtime.</p>
</li>
<li><p><em>--upload-certs</em> -&gt; Faz o upload do certificado do <em>control plane</em> para o kubeadm-certs  secret.</p>
</li>
</ul>
<pre><code>sudo kubeadm init --upload-certs --control-plane-endpoint=ADICIONE_O_IP_DO_NODE_AQUI  --cri-socket /run/containerd/containerd.sock
</code></pre><p>Opcionalmente, você também pode passar o cidr com a opção <em>--pod-network-cidr</em>. O comando obedecerá a seguinte sintaxe:</p>
<pre><code>sudo kubeadm init --upload-certs --control-plane-endpoint=ADICIONE_O_IP_DO_NODE_AQUI  --cri-socket /run/containerd/containerd.sock --pod-network-cidr 192.168.99.0/24
</code></pre><p>A saída do comando será algo similar ao mostrado a seguir.</p>
<pre><code>[init] Using Kubernetes version: v1.24.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;
[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;
[certs] Generating &quot;ca&quot; certificate and key
[certs] Generating &quot;apiserver&quot; certificate and key
[certs] apiserver serving cert is signed for DNS names [172.31.19.147 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.31.19.147]
[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key
[certs] Generating &quot;front-proxy-ca&quot; certificate and key
[certs] Generating &quot;front-proxy-client&quot; certificate and key
[certs] Generating &quot;etcd/ca&quot; certificate and key
[certs] Generating &quot;etcd/server&quot; certificate and key
[certs] etcd/server serving cert is signed for DNS names [172.31.19.147 localhost] and IPs [172.31.19.147 127.0.0.1 ::1]
[certs] Generating &quot;etcd/peer&quot; certificate and key
[certs] etcd/peer serving cert is signed for DNS names [172.31.19.147 localhost] and IPs [172.31.19.147 127.0.0.1 ::1]
[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key
[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key
[certs] Generating &quot;sa&quot; key and public key
[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;
[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file
[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file
[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file
[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;
[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;
[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;
[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;
[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;
[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s
[apiclient] All control plane components are healthy after 8.505808 seconds
[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace
[kubelet] Creating a ConfigMap &quot;kubelet-config&quot; in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Storing the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace
[upload-certs] Using certificate key:
55befb249a01aca7be98b3e7209628f4c4f04c6a05c250c4bb084af722452c36
[mark-control-plane] Marking the node 172.31.19.147 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node 172.31.19.147 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: q1m5ci.5p2mtgby0s4ek4vr
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace
[kubelet-finalize] Updating &quot;/etc/kubernetes/kubelet.conf&quot; to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 172.31.19.147:6443 --token q1m5ci.5p2mtgby0s4ek4vr \
    --discovery-token-ca-cert-hash sha256:45f6437514981d97631bd5d48822c670ec4a548c9768043fca6e5eda0133b934 \
    --control-plane --certificate-key 55befb249a01aca7be98b3e7209628f4c4f04c6a05c250c4bb084af722452c36

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
&quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.31.19.147:6443 --token q1m5ci.5p2mtgby0s4ek4vr \
    --discovery-token-ca-cert-hash sha256:45f6437514981d97631bd5d48822c670ec4a548c9768043fca6e5eda0133b934
</code></pre><p>Caso o servidor possua mais de uma interface de rede, você pode verificar se o IP interno do nó do seu cluster corresponde ao IP da interface esperada com o seguinte comando:</p>
<pre><code>kubectl describe node elliot-1 | grep InternalIP
</code></pre><p>A saída será algo similar a seguir:</p>
<pre><code>InternalIP:  172.31.19.147
</code></pre><p>Caso o IP não corresponda ao da interface de rede escolhida, você pode ir até o arquivo localizado em <em>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</em> com o editor da sua preferência, procure por <em>KUBELET_CONFIG_ARGS</em> e adicione no final a instrução --node-ip=<ip da sua preferência>. O trecho alterado será semelhante a esse:</ip></p>
<pre><code>Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml --node-ip=192.168.99.2&quot;
</code></pre><p>Salve o arquivo e execute os comandos abaixo para reiniciar a configuração e consequentemente o kubelet.</p>
<pre><code>sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre><h4 id="configuração-do-arquivo-de-contextos-do-kubectl">Configuração do arquivo de contextos do kubectl</h4>
<p>Como dito anteriormente e de forma similar ao Docker Swarm, o próprio kubeadm já mostrará os comandos necessários para a configuração do <code>kubectl</code>, para que assim possa ser estabelecida comunicação com o cluster k8s. Para tanto, execute os seguintes comandos.</p>
<pre><code>mkdir -p $HOME/.kube

cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre><h4 id="inserindo-os-nós-workers-no-cluster">Inserindo os nós workers no cluster</h4>
<p>Para inserir os nós <em>workers</em> ou mais <em>control plane</em> no <em>cluster</em>, basta executar a linha que começa com <code>kubeadm join</code> que vimos na saída do comando de inicialização do cluster.</p>
<pre><code>You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 172.31.19.147:6443 --token q1m5ci.5p2mtgby0s4ek4vr \
    --discovery-token-ca-cert-hash sha256:45f6437514981d97631bd5d48822c670ec4a548c9768043fca6e5eda0133b934 \
    --control-plane --certificate-key 55befb249a01aca7be98b3e7209628f4c4f04c6a05c250c4bb084af722452c36

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
&quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.31.19.147:6443 --token q1m5ci.5p2mtgby0s4ek4vr \
    --discovery-token-ca-cert-hash sha256:45f6437514981d97631bd5d48822c670ec4a548c9768043fca6e5eda0133b934
</code></pre><p>Conforme notamos na saída acima temos dois comandos, um para que possamos adicionar mais nós como <em>control plane</em> ou então para adicionar nós como <em>worker</em>.</p>
<p>Apenas copie e cole o comando nos nós que você deseja adicionar ao cluster. Nessa linha de comando do <em>kubeadm join</em> já estamos passando o IP e porta do nosso primeiro nó <em>control plane</em> e as informações sobre o certificado, informações necessárias para que seja possível a entrada do nó no cluster.</p>
<p>Lembre-se, o comando abaixo deve ser executado nos nós que irão compor o cluster, no exemplo vamos adicionar mais dois nós como <em>workers</em></p>
<pre><code class="lang-bash">kubeadm join 172.31.19.147:6443 --token q1m5ci.5p2mtgby0s4ek4vr --discovery-token-ca-cert-hash sha256:45f6437514981d97631bd5d48822c670ec4a548c9768043fca6e5eda0133b934 

[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with <span class="hljs-string">&apos;kubectl -n kube-system get cm kubeadm-config -o yaml&apos;</span>
[kubelet-start] Writing kubelet configuration to file <span class="hljs-string">&quot;/var/lib/kubelet/config.yaml&quot;</span>
[kubelet-start] Writing kubelet environment file with flags to file <span class="hljs-string">&quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span>
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting <span class="hljs-keyword">for</span> the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run <span class="hljs-string">&apos;kubectl get nodes&apos;</span> on the control-plane to see this node join the cluster.
</code></pre>
<p>Agora no nó <em>control plane</em> verifique os nós que já fazem parte do cluster através do comando <em>kubectl</em></p>
<pre><code class="lang-bash">kubectl get nodes
NAME               STATUS     ROLES           AGE   VERSION
ip-172-31-19-147   NotReady   control-plane   68s   v1.24.3
ip-172-31-24-77    NotReady   &lt;none&gt;          29s   v1.24.3
ip-172-31-25-32    NotReady   &lt;none&gt;          31s   v1.24.3
</code></pre>
<p>Perceba que os nós ainda não estão <em>Ready</em>, pois ainda não instalamos o <em>pod network</em> para resolver a comunicação entre pods em diferentes nós.</p>
<h4 id="a-rede-do-kubernetes">A rede do Kubernetes</h4>
<p>Entender como funciona a rede no Kubernetes é super importante para que você consiga entender não somente o comportamento do próprio Kubernetes, como também para o entendimento de como as suas aplicações se comportam e interagem.
Primeira coisa que devemos entender é que o Kubernetes não resolve como funciona a comunicação de pods em nós diferentes, para que isso seja resolvido é necessário utilizar o que chamamos de <em>pod networking</em>.</p>
<p>Ou seja, o k8s por padrão não fornece uma solução de <em>networking</em> <em>out-of-the-box</em>. </p>
<p>Para resolver esse problema foi criado o <em>Container Network Interface</em>, o <strong>CNI</strong>.
O <em>CNI</em> nada mais é do que uma especificação e um conjunto de bibliotecas para a criação de soluções de <em>pod networking</em>, ou seja, plugins para resolver o problema de comunicação entre os pods.</p>
<p>Temos diversas solução de <em>pod networking</em> como <em>add-on</em>, cada qual com funcionalidades diferentes, tais como: <a href="https://github.com/coreos/flannel" target="_blank">Flannel</a>, <a href="http://docs.projectcalico.org/" target="_blank">Calico</a>, <a href="http://romana.io" target="_blank">Romana</a>, <a href="https://www.weave.works/products/weave-net/" target="_blank">Weave-net</a>, entre outros.</p>
<p>É importante saber as caracteristicas de cada solução e como elas resolvem a comunicação entre os pods.</p>
<p>Por exemplo, temos soluções que utilizam <em>eBPF</em> como é o caso do <em>Cilium</em>, ou ainda soluções que atuam na camada 3 ou na camada 7 do modelo de referencia OSI. </p>
<p>Dito isso, a melhor coisa é você ler os detalhes de cada solução e entender qual a melhor antende suas necessidades.</p>
<p>Eu gosto muito da <strong>Weave-net</strong> e será ela que iremos abordar durante o treinamento, na dúvida de qual usar, vá de <strong>Weave-net</strong>! :)</p>
<p>Para instalar o <em>Weave-net</em> execute o seguinte comando no nó <em>control plane</em>.</p>
<pre><code>kubectl apply -f &quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d &apos;\n&apos;)&quot;
</code></pre><p>Para verificar se o <em>pod network</em> foi criado com sucesso, execute o seguinte comando.</p>
<pre><code>kubectl get pods -n kube-system
</code></pre><p>O resultado deve ser semelhante ao mostrado a seguir.</p>
<pre><code>NAMESPACE     NAME                                       READY   STATUS    RESTARTS      AGE
kube-system   coredns-6d4b75cb6d-vjtw5                   1/1     Running   0             2m4s
kube-system   coredns-6d4b75cb6d-xd89l                   1/1     Running   0             2m4s
kube-system   etcd-ip-172-31-19-147                      1/1     Running   0             2m19s
kube-system   kube-apiserver-ip-172-31-19-147            1/1     Running   0             2m18s
kube-system   kube-controller-manager-ip-172-31-19-147   1/1     Running   0             2m18s
kube-system   kube-proxy-djvp4                           1/1     Running   0             103s
kube-system   kube-proxy-f2f57                           1/1     Running   0             2m5s
kube-system   kube-proxy-tshff                           1/1     Running   0             105s
kube-system   kube-scheduler-ip-172-31-19-147            1/1     Running   0             2m18s
kube-system   weave-net-4qfbb                            2/2     Running   1 (22s ago)   28s
kube-system   weave-net-htlrp                            2/2     Running   1 (22s ago)   28s
kube-system   weave-net-nltmv                            2/2     Running   1 (21s ago)   28s
</code></pre><p>Pode-se observar que há três contêineres do Weave-net em execução, um em cada nó do cluster,  provendo a <em>pod networking</em> para o nosso <em>cluster</em>.</p>
<h4 id="verificando-a-instalação">Verificando a instalação</h4>
<p>Para verificar se a instalação está funcionando, e se os nós estão se comunicando, você pode executar o comando <code>kubectl get nodes</code> no nó control-plane, que deve lhe retornar algo como o conteúdo a seguir.</p>
<pre><code class="lang-bash">kubectl get nodes

NAME               STATUS   ROLES           AGE     VERSION
ip-172-31-19-147   Ready    control-plane   2m20s   v1.24.3
ip-172-31-24-77    Ready    &lt;none&gt;          101s    v1.24.3
ip-172-31-25-32    Ready    &lt;none&gt;          103s    v1.24.3
</code></pre>
<h3 id="primeiros-passos-no-k8s">Primeiros passos no k8s</h3>
<h4 id="exibindo-informações-detalhadas-sobre-os-nós">Exibindo informações detalhadas sobre os nós</h4>
<pre><code>kubectl describe node [nome_do_no]
</code></pre><p>Exemplo:</p>
<pre><code>kubectl describe node ip-172-31-19-147

Name:               ip-172-31-19-147
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=ip-172-31-19-147
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 07 Aug 2022 07:05:52 +0000
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
                    node-role.kubernetes.io/master:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  ip-172-31-19-147
  AcquireTime:     &lt;unset&gt;
  RenewTime:       Sun, 07 Aug 2022 08:10:33 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Sun, 07 Aug 2022 07:07:56 +0000   Sun, 07 Aug 2022 07:07:56 +0000   WeaveIsUp                    Weave pod has set this
  MemoryPressure       False   Sun, 07 Aug 2022 08:09:15 +0000   Sun, 07 Aug 2022 07:05:49 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Sun, 07 Aug 2022 08:09:15 +0000   Sun, 07 Aug 2022 07:05:49 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Sun, 07 Aug 2022 08:09:15 +0000   Sun, 07 Aug 2022 07:05:49 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Sun, 07 Aug 2022 08:09:15 +0000   Sun, 07 Aug 2022 07:07:58 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  172.31.19.147
  Hostname:    ip-172-31-19-147
Capacity:
  cpu:                2
  ephemeral-storage:  7950756Ki
  hugepages-2Mi:      0
  memory:             4016852Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  7327416718
  hugepages-2Mi:      0
  memory:             3914452Ki
  pods:               110
System Info:
  Machine ID:                 23fb437f79c4489ab1e351f42b69a52c
  System UUID:                ec2e1b61-092b-df48-4c41-f51d2f5e84d7
  Boot ID:                    1e1ce6a2-3cf0-4961-be37-1f15ba5cd232
  Kernel Version:             5.13.0-1029-aws
  OS Image:                   Ubuntu 20.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.6
  Kubelet Version:            v1.24.3
  Kube-Proxy Version:         v1.24.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
....
</code></pre><h5 id="exibindo-novamente-token-para-adicionar-um-novo-nó-no-cluster">Exibindo novamente token para adicionar um novo nó no cluster</h5>
<p>Para visualizar novamente o <em>token</em> para inserção de novos nós, execute o seguinte comando.</p>
<pre><code>sudo kubeadm token create --print-join-command
</code></pre><h5 id="ativando-o-autocomplete">Ativando o autocomplete</h5>
<p>Em distribuições Debian e baseadas, certifique-se que o pacote <code>bash-completion</code> esteja instalado. Instale-o com o comando a seguir.</p>
<pre><code>sudo apt install -y bash-completion
</code></pre><p>Em sistemas Red Hat e baseados, execute:</p>
<pre><code>sudo yum install -y bash-completion
</code></pre><p>Feito isso, execute o seguinte comando.</p>
<pre><code>kubectl completion bash &gt; /etc/bash_completion.d/kubectl
</code></pre><p>Efetue <em>logoff</em> e <em>login</em> para carregar o <em>autocomplete</em>. Caso não deseje, execute:</p>
<pre><code>source &lt;(kubectl completion bash)
</code></pre><h5 id="verificando-os-namespaces-e-pods">Verificando os namespaces e pods</h5>
<p>O k8s organiza tudo dentro de <em>namespaces</em>. Por meio deles, podem ser realizadas limitações de segurança e de recursos dentro do <em>cluster</em>, tais como <em>pods</em>, <em>replication controllers</em> e diversos outros. Para visualizar os <em>namespaces</em> disponíveis no <em>cluster</em>, digite:</p>
<pre><code>kubectl get namespaces

NAME              STATUS   AGE
default           Active   8d
kube-node-lease   Active   8d
kube-public       Active   8d
kube-system       Active   8d
</code></pre><p>Vamos listar os <em>pods</em> do <em>namespace</em> <strong>kube-system</strong> utilizando o comando a seguir.</p>
<pre><code>kubectl get pod -n kube-system

NAME                                       READY   STATUS    RESTARTS       AGE
coredns-6d4b75cb6d-vjtw5                   1/1     Running   0              106m
coredns-6d4b75cb6d-xd89l                   1/1     Running   0              106m
etcd-ip-172-31-19-147                      1/1     Running   0              106m
kube-apiserver-ip-172-31-19-147            1/1     Running   0              106m
kube-controller-manager-ip-172-31-19-147   1/1     Running   0              106m
kube-proxy-djvp4                           1/1     Running   0              106m
kube-proxy-f2f57                           1/1     Running   0              106m
kube-proxy-tshff                           1/1     Running   0              106m
kube-scheduler-ip-172-31-19-147            1/1     Running   0              106m
weave-net-4qfbb                            2/2     Running   1 (104m ago)   105m
weave-net-htlrp                            2/2     Running   1 (104m ago)   105m
weave-net-nltmv                            2/2     Running   1 (104m ago)   105m
</code></pre><p>Será que há algum <em>pod</em> escondido em algum <em>namespace</em>? É possível listar todos os <em>pods</em> de todos os <em>namespaces</em> com o comando a seguir.</p>
<pre><code>kubectl get pods --all-namespaces
</code></pre><p>Há a possibilidade ainda, de utilizar o comando com a opção <code>-o wide</code>, que disponibiliza maiores informações sobre o recurso, inclusive em qual nó o <em>pod</em> está sendo executado. Exemplo:</p>
<pre><code>kubectl get pods --all-namespaces -o wide

NAMESPACE     NAME                                       READY   STATUS    RESTARTS       AGE    IP              NODE               NOMINATED NODE   READINESS GATES
kube-system   coredns-6d4b75cb6d-vjtw5                   1/1     Running   0              105m   10.32.0.3       ip-172-31-19-147   &lt;none&gt;           &lt;none&gt;
kube-system   coredns-6d4b75cb6d-xd89l                   1/1     Running   0              105m   10.32.0.2       ip-172-31-19-147   &lt;none&gt;           &lt;none&gt;
kube-system   etcd-ip-172-31-19-147                      1/1     Running   0              105m   172.31.19.147   ip-172-31-19-147   &lt;none&gt;           &lt;none&gt;
kube-system   kube-apiserver-ip-172-31-19-147            1/1     Running   0              105m   172.31.19.147   ip-172-31-19-147   &lt;none&gt;           &lt;none&gt;
kube-system   kube-controller-manager-ip-172-31-19-147   1/1     Running   0              105m   172.31.19.147   ip-172-31-19-147   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-djvp4                           1/1     Running   0              105m   172.31.24.77    ip-172-31-24-77    &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-f2f57                           1/1     Running   0              105m   172.31.19.147   ip-172-31-19-147   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-tshff                           1/1     Running   0              105m   172.31.25.32    ip-172-31-25-32    &lt;none&gt;           &lt;none&gt;
kube-system   kube-scheduler-ip-172-31-19-147            1/1     Running   0              105m   172.31.19.147   ip-172-31-19-147   &lt;none&gt;           &lt;none&gt;
kube-system   weave-net-4qfbb                            2/2     Running   1 (103m ago)   103m   172.31.19.147   ip-172-31-19-147   &lt;none&gt;           &lt;none&gt;
kube-system   weave-net-htlrp                            2/2     Running   1 (103m ago)   103m   172.31.25.32    ip-172-31-25-32    &lt;none&gt;           &lt;none&gt;
kube-system   weave-net-nltmv                            2/2     Running   1 (103m ago)   103m   172.31.24.77    ip-172-31-24-77    &lt;none&gt;           &lt;none&gt;
</code></pre><h5 id="executando-nosso-primeiro-pod-no-k8s">Executando nosso primeiro pod no k8s</h5>
<p>Iremos iniciar o nosso primeiro <em>pod</em> no k8s. Para isso, executaremos o comando a seguir.</p>
<pre><code>kubectl run nginx --image nginx

pod/nginx created
</code></pre><p>Listando os <em>pods</em> com <code>kubectl get pods</code>, obteremos a seguinte saída.</p>
<pre><code>NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          66s
</code></pre><p>Vamos olhar agora a descrição desse objeto dentro do <em>cluster</em>.</p>
<pre><code>kubectl describe pod nginx

Name:         nginx
Namespace:    default
Priority:     0
Node:         ip-172-31-25-32/172.31.25.32
Start Time:   Sun, 07 Aug 2022 08:53:24 +0000
Labels:       run=nginx
Annotations:  &lt;none&gt;
Status:       Running
IP:           10.40.0.1
IPs:
  IP:  10.40.0.1
Containers:
  nginx:
    Container ID:   containerd://d7ae9933e65477eed7ff04a107fb3a3adb6a634bc713282421bbdf0e1c30bf7b
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:ecc068890de55a75f1a32cc8063e79f90f0b043d70c5fcf28f1713395a4b3d49
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Sun, 07 Aug 2022 08:53:30 +0000
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tmjgq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-tmjgq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  16s   default-scheduler  Successfully assigned default/nginx to ip-172-31-25-32
  Normal  Pulling    16s   kubelet            Pulling image &quot;nginx&quot;
  Normal  Pulled     10s   kubelet            Successfully pulled image &quot;nginx&quot; in 5.387864178s
  Normal  Created    10s   kubelet            Created container nginx
  Normal  Started    10s   kubelet            Started container nginx
</code></pre><h5 id="verificar-os-últimos-eventos-do-cluster">Verificar os últimos eventos do cluster</h5>
<p>Você pode verificar quais são os últimos eventos do <em>cluster</em> com o comando <code>kubectl get events</code>. Serão mostrados eventos como: o <em>download</em> de imagens do Docker Hub (ou de outro <em>registry</em> configurado), a criação/remoção de <em>pods</em>, etc.</p>
<p>A saída a seguir mostra o resultado da criação do nosso contêiner com Nginx.</p>
<pre><code>kubectl get events

LAST SEEN   TYPE     REASON      OBJECT      MESSAGE
44s         Normal   Scheduled   pod/nginx   Successfully assigned default/nginx to ip-172-31-25-32
44s         Normal   Pulling     pod/nginx   Pulling image &quot;nginx&quot;
38s         Normal   Pulled      pod/nginx   Successfully pulled image &quot;nginx&quot; in 5.387864178s
38s         Normal   Created     pod/nginx   Created container nginx
38s         Normal   Started     pod/nginx   Started container nginx
</code></pre><p>No resultado do comando anterior é possível observar que a execução do nginx ocorreu no <em>namespace</em> default e que a imagem <strong>nginx</strong> não existia no repositório local e, sendo assim, teve de ser feito download da imagem.</p>
<h5 id="efetuar-o-dump-de-um-objeto-em-formato-yaml">Efetuar o dump de um objeto em formato YAML</h5>
<p>Assim como quando se está trabalhando com <em>stacks</em> no Docker Swarm, normalmente recursos no k8s são declarados em arquivos <strong>YAML</strong> ou <strong>JSON</strong> e depois manipulados através do <code>kubectl</code>.</p>
<p>Para nos poupar o trabalho de escrever o arquivo inteiro, pode-se utilizar como <em>template</em> o <em>dump</em> de um objeto já existente no k8s, como mostrado a seguir.</p>
<pre><code>kubectl get pod nginx -o yaml &gt; meu-primeiro.yaml
</code></pre><p>Será criado um novo arquivo chamado <code>meu-primeiro.yaml</code>, resultante do redirecionamento da saída do comando <code>kubectl get pod nginx -o yaml</code>.</p>
<p>Abrindo o arquivo com <code>vim meu-primeiro.yaml</code> (você pode utilizar o editor que você preferir), teremos o seguinte conteúdo.</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">creationTimestamp:</span> <span class="hljs-string">&quot;2022-08-07T08:53:24Z&quot;</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">run:</span> <span class="hljs-string">nginx</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>
  <span class="hljs-attr">resourceVersion:</span> <span class="hljs-string">&quot;9598&quot;</span>
  <span class="hljs-attr">uid:</span> <span class="hljs-string">d0928186-bf6d-459b-aca6-9b0d84b40e9c</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span>
    <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">Always</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
    <span class="hljs-attr">resources:</span> {}
    <span class="hljs-attr">terminationMessagePath:</span> <span class="hljs-string">/dev/termination-log</span>
    <span class="hljs-attr">terminationMessagePolicy:</span> <span class="hljs-string">File</span>
    <span class="hljs-attr">volumeMounts:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/run/secrets/kubernetes.io/serviceaccount</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">kube-api-access-tmjgq</span>
      <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">dnsPolicy:</span> <span class="hljs-string">ClusterFirst</span>
  <span class="hljs-attr">enableServiceLinks:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">nodeName:</span> <span class="hljs-string">ip-172-31-25-32</span>
  <span class="hljs-attr">preemptionPolicy:</span> <span class="hljs-string">PreemptLowerPriority</span>
  <span class="hljs-attr">priority:</span> <span class="hljs-number">0</span>
  <span class="hljs-attr">restartPolicy:</span> <span class="hljs-string">Always</span>
  <span class="hljs-attr">schedulerName:</span> <span class="hljs-string">default-scheduler</span>
  <span class="hljs-attr">securityContext:</span> {}
  <span class="hljs-attr">serviceAccount:</span> <span class="hljs-string">default</span>
  <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">default</span>
  <span class="hljs-attr">terminationGracePeriodSeconds:</span> <span class="hljs-number">30</span>
  <span class="hljs-attr">tolerations:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoExecute</span>
    <span class="hljs-attr">key:</span> <span class="hljs-string">node.kubernetes.io/not-ready</span>
    <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span>
    <span class="hljs-attr">tolerationSeconds:</span> <span class="hljs-number">300</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoExecute</span>
    <span class="hljs-attr">key:</span> <span class="hljs-string">node.kubernetes.io/unreachable</span>
    <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span>
    <span class="hljs-attr">tolerationSeconds:</span> <span class="hljs-number">300</span>
  <span class="hljs-attr">volumes:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">kube-api-access-tmjgq</span>
    <span class="hljs-attr">projected:</span>
      <span class="hljs-attr">defaultMode:</span> <span class="hljs-number">420</span>
      <span class="hljs-attr">sources:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">serviceAccountToken:</span>
          <span class="hljs-attr">expirationSeconds:</span> <span class="hljs-number">3607</span>
          <span class="hljs-attr">path:</span> <span class="hljs-string">token</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">configMap:</span>
          <span class="hljs-attr">items:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">ca.crt</span>
            <span class="hljs-attr">path:</span> <span class="hljs-string">ca.crt</span>
          <span class="hljs-attr">name:</span> <span class="hljs-string">kube-root-ca.crt</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">downwardAPI:</span>
          <span class="hljs-attr">items:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">fieldRef:</span>
              <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
              <span class="hljs-attr">fieldPath:</span> <span class="hljs-string">metadata.namespace</span>
            <span class="hljs-attr">path:</span> <span class="hljs-string">namespace</span>
<span class="hljs-attr">status:</span>
  <span class="hljs-attr">conditions:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">lastProbeTime:</span> <span class="hljs-literal">null</span>
    <span class="hljs-attr">lastTransitionTime:</span> <span class="hljs-string">&quot;2022-08-07T08:53:24Z&quot;</span>
    <span class="hljs-attr">status:</span> <span class="hljs-string">&quot;True&quot;</span>
    <span class="hljs-attr">type:</span> <span class="hljs-string">Initialized</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">lastProbeTime:</span> <span class="hljs-literal">null</span>
    <span class="hljs-attr">lastTransitionTime:</span> <span class="hljs-string">&quot;2022-08-07T08:53:30Z&quot;</span>
    <span class="hljs-attr">status:</span> <span class="hljs-string">&quot;True&quot;</span>
    <span class="hljs-attr">type:</span> <span class="hljs-string">Ready</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">lastProbeTime:</span> <span class="hljs-literal">null</span>
    <span class="hljs-attr">lastTransitionTime:</span> <span class="hljs-string">&quot;2022-08-07T08:53:30Z&quot;</span>
    <span class="hljs-attr">status:</span> <span class="hljs-string">&quot;True&quot;</span>
    <span class="hljs-attr">type:</span> <span class="hljs-string">ContainersReady</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">lastProbeTime:</span> <span class="hljs-literal">null</span>
    <span class="hljs-attr">lastTransitionTime:</span> <span class="hljs-string">&quot;2022-08-07T08:53:24Z&quot;</span>
    <span class="hljs-attr">status:</span> <span class="hljs-string">&quot;True&quot;</span>
    <span class="hljs-attr">type:</span> <span class="hljs-string">PodScheduled</span>
  <span class="hljs-attr">containerStatuses:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">containerID:</span> <span class="hljs-string">containerd://d7ae9933e65477eed7ff04a107fb3a3adb6a634bc713282421bbdf0e1c30bf7b</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">docker.io/library/nginx:latest</span>
    <span class="hljs-attr">imageID:</span> <span class="hljs-string">docker.io/library/nginx@sha256:ecc068890de55a75f1a32cc8063e79f90f0b043d70c5fcf28f1713395a4b3d49</span>
    <span class="hljs-attr">lastState:</span> {}
    <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
    <span class="hljs-attr">ready:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">restartCount:</span> <span class="hljs-number">0</span>
    <span class="hljs-attr">started:</span> <span class="hljs-literal">true</span>
    <span class="hljs-attr">state:</span>
      <span class="hljs-attr">running:</span>
        <span class="hljs-attr">startedAt:</span> <span class="hljs-string">&quot;2022-08-07T08:53:30Z&quot;</span>
  <span class="hljs-attr">hostIP:</span> <span class="hljs-number">172.31</span><span class="hljs-number">.25</span><span class="hljs-number">.32</span>
  <span class="hljs-attr">phase:</span> <span class="hljs-string">Running</span>
  <span class="hljs-attr">podIP:</span> <span class="hljs-number">10.40</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>
  <span class="hljs-attr">podIPs:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">ip:</span> <span class="hljs-number">10.40</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>
  <span class="hljs-attr">qosClass:</span> <span class="hljs-string">BestEffort</span>
  <span class="hljs-attr">startTime:</span> <span class="hljs-string">&quot;2022-08-07T08:53:24Z&quot;</span>
</code></pre>
<p>Observando o arquivo anterior, notamos que este reflete o <strong>estado</strong> do <em>pod</em>. Nós desejamos utilizar tal arquivo apenas como um modelo, e sendo assim, podemos apagar as entradas que armazenam dados de estado desse <em>pod</em>, como <em>status</em> e todas as demais configurações que são específicas dele. O arquivo final ficará com o conteúdo semelhante a este:</p>
<pre><code class="lang-yaml">  <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
  <span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
  <span class="hljs-attr">metadata:</span>
    <span class="hljs-attr">labels:</span>
      <span class="hljs-attr">run:</span> <span class="hljs-string">nginx</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
  <span class="hljs-attr">spec:</span>
    <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">resources:</span> {}
    <span class="hljs-attr">dnsPolicy:</span> <span class="hljs-string">ClusterFirst</span>
    <span class="hljs-attr">restartPolicy:</span> <span class="hljs-string">Always</span>
  <span class="hljs-attr">status:</span> {}
</code></pre>
<p>Vamos agora remover o nosso <em>pod</em> com o seguinte comando.</p>
<pre><code>kubectl delete pod nginx
</code></pre><p>A saída deve ser algo como:</p>
<pre><code>pod &quot;nginx&quot; deleted
</code></pre><p>Vamos recriá-lo, agora a partir do nosso arquivo YAML.</p>
<pre><code>kubectl create -f meu-primeiro.yaml

pod/nginx created
</code></pre><p>Observe que não foi necessário informar ao <code>kubectl</code> qual tipo de recurso seria criado, pois isso já está contido dentro do arquivo.</p>
<p>Listando os <em>pods</em> disponíveis com o seguinte comando.</p>
<pre><code>kubectl get pods
</code></pre><p>Deve-se obter uma saída similar à esta:</p>
<pre><code>NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          109s
</code></pre><p>Uma outra forma de criar um arquivo de <em>template</em> é através da opção <code>--dry-run</code> do <code>kubectl</code>, com o funcionamento ligeiramente diferente dependendo do tipo de recurso que será criado. Exemplos:</p>
<p>Para a criação do template de um <em>pod</em>:</p>
<pre><code>kubectl run meu-nginx --image nginx --dry-run=client -o yaml &gt; pod-template.yaml
</code></pre><p>Para a criação do <em>template</em> de um <em>deployment</em>:</p>
<pre><code>kubectl create deployment meu-nginx --image=nginx --dry-run=client -o yaml &gt; deployment-template.yaml
</code></pre><p>A vantagem deste método é que não há a necessidade de limpar o arquivo, além de serem apresentadas apenas as opções necessárias do recurso.</p>
<h4 id="socorro-são-muitas-opções">Socorro, são muitas opções!</h4>
<p>Calma, nós sabemos. Mas o <code>kubectl</code> pode lhe auxiliar um pouco em relação a isso. Ele contém a opção <code>explain</code>, que você pode utilizar caso precise de ajuda com alguma opção em específico dos arquivos de recurso. A seguir alguns exemplos de sintaxe.</p>
<pre><code>kubectl explain [recurso]

kubectl explain [recurso.caminho.para.spec]

kubectl explain [recurso.caminho.para.spec] --recursive
</code></pre><p>Exemplos:</p>
<pre><code>kubectl explain deployment

kubectl explain pod --recursive

kubectl explain deployment.spec.template.spec
</code></pre><h4 id="expondo-o-pod-e-criando-um-service">Expondo o pod e criando um Service</h4>
<p>Dispositivos fora do <em>cluster</em>, por padrão, não conseguem acessar os <em>pods</em> criados, como é comum em outros sistemas de contêineres. Para expor um <em>pod</em>, execute o comando a seguir.</p>
<pre><code>kubectl expose pod nginx
</code></pre><p>Será apresentada a seguinte mensagem de erro:</p>
<pre><code>error: couldn&apos;t find port via --port flag or introspection
See &apos;kubectl expose -h&apos; for help and examples
</code></pre><p>O erro ocorre devido ao fato do k8s não saber qual é a porta de destino do contêiner que deve ser exposta (no caso, a 80/TCP). Para configurá-la, vamos primeiramente remover o nosso <em>pod</em> antigo:</p>
<pre><code>kubectl delete -f meu-primeiro.yaml
</code></pre><p>Abra agora o arquivo <code>meu-primeiro.yaml</code> e adicione o bloco a seguir.</p>
<pre><code class="lang-yaml"><span class="hljs-string">...</span>
<span class="hljs-attr">spec:</span>
       <span class="hljs-attr">containers:</span>
       <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span>
         <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">Always</span>
         <span class="hljs-attr">ports:</span>
         <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span>
         <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
         <span class="hljs-attr">resources:</span> {}
<span class="hljs-string">...</span>
</code></pre>
<blockquote>
<p><strong>Atenção!!!</strong> Arquivos YAML utilizam para sua tabulação dois espaços e não <em>tab</em>.</p>
</blockquote>
<p>Feita a modificação no arquivo, salve-o e crie novamente o <em>pod</em> com o comando a seguir.</p>
<pre><code>kubectl create -f meu-primeiro.yaml

pod/nginx created
</code></pre><p>Liste o pod.</p>
<pre><code>kubectl get pod nginx

NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          32s
</code></pre><p>O comando a seguir cria um objeto do k8s chamado de <em>Service</em>, que é utilizado justamente para expor <em>pods</em> para acesso externo.</p>
<pre><code>kubectl expose pod nginx
</code></pre><p>Podemos listar todos os <em>services</em> com o comando a seguir.</p>
<pre><code>kubectl get services

NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP   8d
nginx        ClusterIP   10.105.41.192   &lt;none&gt;        80/TCP    2m30s
</code></pre><p>Como é possível observar, há dois <em>services</em> no nosso <em>cluster</em>: o primeiro é para uso do próprio k8s, enquanto o segundo foi o quê acabamos de criar. Utilizando o <code>curl</code> contra o endereço IP mostrado na coluna <em>CLUSTER-IP</em>, deve nos ser apresentada a tela principal do Nginx.</p>
<pre><code>curl 10.105.41.192

&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre><p>Este <em>pod</em> está disponível para acesso a partir de qualquer nó do <em>cluster</em>.</p>
<h4 id="limpando-tudo-e-indo-para-casa">Limpando tudo e indo para casa</h4>
<p>Para mostrar todos os recursos recém criados, pode-se utilizar uma das seguintes opções a seguir.</p>
<pre><code>kubectl get all

kubectl get pod,service

kubectl get pod,svc
</code></pre><p>Note que o k8s nos disponibiliza algumas abreviações de seus recursos. Com o tempo você irá se familiar com elas. Para apagar os recursos criados, você pode executar os seguintes comandos.</p>
<pre><code>kubectl delete -f meu-primeiro.yaml

kubectl delete service nginx
</code></pre><p>Liste novamente os recursos para ver se os mesmos ainda estão presentes.</p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="../" class="navigation navigation-prev " aria-label="Previous page: Introdução">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="../day_two/descomplicando_kubernetes.html" class="navigation navigation-next " aria-label="Next page: Descomplicando Kubernetes dia 2">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Descomplicando Kubernetes dia 1","level":"2.1","depth":1,"next":{"title":"Descomplicando Kubernetes dia 2","level":"2.2","depth":1,"path":"day_two/descomplicando_kubernetes.md","ref":"day_two/descomplicando_kubernetes.md","articles":[]},"previous":{"title":"Introdução","level":"1.1","depth":1,"path":"README.md","ref":"README.md","articles":[]},"dir":"ltr"},"config":{"plugins":[],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"theme":"default","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56},"embedFonts":false},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"language":"pt","gitbook":"*"},"file":{"path":"day_one/descomplicando_kubernetes.md","mtime":"2022-12-28T13:27:41.479Z","type":"markdown"},"gitbook":{"version":"3.6.20","time":"2022-12-28T13:27:52.461Z"},"basePath":"..","book":{"language":"pt"}});
        });
    </script>
</div>

        
    <noscript>
        <style>
            .honkit-cloak {
                display: block !important;
            }
        </style>
    </noscript>
    <script>
        // Restore sidebar state as critical path for prevent layout shift
        function __init__getSidebarState(defaultValue){
            var baseKey = "";
            var key = baseKey + ":sidebar";
            try {
                var value = localStorage[key];
                if (value === undefined) {
                    return defaultValue;
                }
                var parsed = JSON.parse(value);
                return parsed == null ? defaultValue : parsed;
            } catch (e) {
                return defaultValue;
            }
        }
        function __init__restoreLastSidebarState() {
            var isMobile = window.matchMedia("(max-width: 600px)").matches;
            if (isMobile) {
                // Init last state if not mobile
                return;
            }
            var sidebarState = __init__getSidebarState(true);
            var book = document.querySelector(".book");
            // Show sidebar if it enabled
            if (sidebarState && book) {
                book.classList.add("without-animation", "with-summary");
            }
        }

        try {
            __init__restoreLastSidebarState();
        } finally {
            var book = document.querySelector(".book");
            book.classList.remove("honkit-cloak");
        }
    </script>
    <script src="../../gitbook/gitbook.js"></script>
    <script src="../../gitbook/theme.js"></script>
    
        
        <script src="../../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

